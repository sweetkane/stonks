{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.imports import *\n",
    "from common.util import *\n",
    "from common.const import *\n",
    "from data.dataloader import *\n",
    "from model.transformer import *\n",
    "from model.run_model import *\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN SETTINGS\n",
    "max_src_list = [100]\n",
    "test_size = 20\n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "num_workers = 0\n",
    "days_pred_list = [50]\n",
    "csv_file_name = 'data/csv/tuning_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kanesweet/anaconda3/envs/stonksEnv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:218: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because num_head is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# OBJECTS\n",
    "\n",
    "## DATALOADER\n",
    "train_loader, test_loader = create_splits(\n",
    "    batch_size=batch_size,\n",
    "    test_size=test_size,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "## BATCH PROCESSOR\n",
    "batch_processor = BatchProcessor(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n",
      "bad loss\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m max_norm \u001b[39min\u001b[39;00m [\u001b[39m1000\u001b[39m,\u001b[39m10000\u001b[39m,\u001b[39m100000\u001b[39m]:\n\u001b[1;32m     21\u001b[0m     \u001b[39mfor\u001b[39;00m learning_rate \u001b[39min\u001b[39;00m [\u001b[39m0.01\u001b[39m,\u001b[39m0.025\u001b[39m,\u001b[39m0.05\u001b[39m,\u001b[39m0.1\u001b[39m]:\n\u001b[0;32m---> 22\u001b[0m         all_losses \u001b[39m=\u001b[39m train(learning_rate, max_norm)\n\u001b[1;32m     23\u001b[0m         loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(all_losses[\u001b[39m-\u001b[39m\u001b[39m100\u001b[39m:])\n\u001b[1;32m     24\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(csv_file_name, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, newline\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(lr, max_norm)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m days_pred \u001b[39min\u001b[39;00m days_pred_list:\n\u001b[1;32m      8\u001b[0m         \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      9\u001b[0m             all_losses \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((\n\u001b[1;32m     10\u001b[0m                 all_losses,\n\u001b[0;32m---> 11\u001b[0m                 train_stonks_transformer(\n\u001b[1;32m     12\u001b[0m                     model, lr, max_norm, days_pred, train_loader, \n\u001b[1;32m     13\u001b[0m                     batch_processor, device\n\u001b[1;32m     14\u001b[0m                 )\n\u001b[1;32m     15\u001b[0m             ))\n\u001b[1;32m     16\u001b[0m \u001b[39mreturn\u001b[39;00m all_losses\n",
      "File \u001b[0;32m~/kanesweet/stonks.0.2/model/run_model.py:43\u001b[0m, in \u001b[0;36mtrain_stonks_transformer\u001b[0;34m(model, learning_rate, max_norm, days_pred, train_loader, batch_processor, device, pbar)\u001b[0m\n\u001b[1;32m     41\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     42\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), max_norm)\n\u001b[0;32m---> 43\u001b[0m     opt\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(losses)\n",
      "File \u001b[0;32m~/anaconda3/envs/stonksEnv/lib/python3.11/site-packages/torch/optim/optimizer.py:295\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    296\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    298\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/stonksEnv/lib/python3.11/site-packages/torch/optim/optimizer.py:37\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 37\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     38\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/stonksEnv/lib/python3.11/site-packages/torch/optim/adamw.py:176\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    163\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    166\u001b[0m         group,\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         state_steps,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 176\u001b[0m     adamw(\n\u001b[1;32m    177\u001b[0m         params_with_grad,\n\u001b[1;32m    178\u001b[0m         grads,\n\u001b[1;32m    179\u001b[0m         exp_avgs,\n\u001b[1;32m    180\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    181\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    182\u001b[0m         state_steps,\n\u001b[1;32m    183\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    184\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    185\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    186\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    187\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    188\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    189\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    190\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    191\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    192\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    193\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    194\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    195\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    198\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/stonksEnv/lib/python3.11/site-packages/torch/optim/adamw.py:326\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 326\u001b[0m func(\n\u001b[1;32m    327\u001b[0m     params,\n\u001b[1;32m    328\u001b[0m     grads,\n\u001b[1;32m    329\u001b[0m     exp_avgs,\n\u001b[1;32m    330\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    331\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    332\u001b[0m     state_steps,\n\u001b[1;32m    333\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    334\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    335\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    336\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    337\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    338\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    339\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    340\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    341\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    342\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    343\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[1;32m    344\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/stonksEnv/lib/python3.11/site-packages/torch/optim/adamw.py:498\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    495\u001b[0m device_params \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mview_as_real(x) \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_complex(x) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m device_params]\n\u001b[1;32m    497\u001b[0m \u001b[39m# update steps\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m torch\u001b[39m.\u001b[39;49m_foreach_add_(device_state_steps, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    500\u001b[0m \u001b[39m# Perform stepweight decay\u001b[39;00m\n\u001b[1;32m    501\u001b[0m torch\u001b[39m.\u001b[39m_foreach_mul_(device_params, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m lr \u001b[39m*\u001b[39m weight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "\n",
    "def train(model, lr, max_norm):\n",
    "    all_losses = np.empty((0,))\n",
    "    for max_src in max_src_list:\n",
    "        batch_processor.max_src_window = max_src\n",
    "        for days_pred in days_pred_list:\n",
    "            for _ in range(num_epochs):\n",
    "                all_losses = np.concatenate((\n",
    "                    all_losses,\n",
    "                    train_stonks_transformer(\n",
    "                        model, lr, max_norm, days_pred, train_loader, \n",
    "                        batch_processor, device\n",
    "                    )\n",
    "                ))\n",
    "    return all_losses\n",
    "\n",
    "\n",
    "for max_norm in [1000,10000,100000]:\n",
    "    for learning_rate in [0.01,0.025,0.05,0.1]:\n",
    "        model = stonks_transformer_model().to(device)\n",
    "        all_losses = train(model, learning_rate, max_norm)\n",
    "        loss = np.mean(all_losses[-100:])\n",
    "        with open(csv_file_name, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(\n",
    "                [loss,learning_rate,max_norm,batch_size,num_epochs]\n",
    "            )\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stonksEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
